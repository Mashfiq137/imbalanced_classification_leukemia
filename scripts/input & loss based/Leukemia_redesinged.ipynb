{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b1bd5615",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os \n",
    "# os.environ['KMP_DUPLICATE_LIB_OK']='True'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed2a589e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c1ee8e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "for gpu in gpus:\n",
    "    print(\"Name:\", gpu.name, \"  Type:\", gpu.device_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c99b9828",
   "metadata": {},
   "outputs": [],
   "source": [
    "# '''\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.client import device_lib\n",
    "def get_available_gpus():\n",
    "    local_device_protos = device_lib.list_local_devices()\n",
    "    return [x.name for x in local_device_protos if x.device_type == 'GPU']\n",
    "\n",
    "print(get_available_gpus())\n",
    "# '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4e9a5a53",
   "metadata": {},
   "outputs": [],
   "source": [
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "try:\n",
    "  tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "except:\n",
    "    print(\"Problem\")\n",
    "  # Invalid device or cannot modify virtual devices once initialized.\n",
    "#   pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "59db683a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "physical_devices = tf.config.list_physical_devices('GPU') \n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e10dc88",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ef94c0e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline    \n",
    "import operator   \n",
    "import tensorflow as tf\n",
    "import random\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "import os\n",
    "import skimage\n",
    "from skimage.io import imread, imshow\n",
    "#import ktrain\n",
    "\n",
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline    \n",
    "import operator   \n",
    "import tensorflow as tf\n",
    "import random\n",
    "import skimage\n",
    "from skimage.io import imread, imshow\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "# %reload_ext autoreload\n",
    "# %autoreload 2\n",
    "# %matplotlib inline\n",
    "# import os\n",
    "# import ktrain\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "77e05db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAIN_DIRECTORY = r'F:\\Leuk study re-designed\\C-NMC\\Low imbalance\\Aug 5x'\n",
    "\n",
    "# CATEGORIES = ['all', 'hem']\n",
    "\n",
    "height = 210\n",
    "width = 210\n",
    "crop = 210\n",
    "\n",
    "best_model_name = 'HRD_EfficientNetB0_MOd_Mcc_loss_test1.h5' \n",
    "factor=0.93\n",
    "patience=2\n",
    "epoch=200\n",
    "\n",
    "#'''\n",
    "'''Baseline path'''\n",
    "TRAIN_PATH = r'F:\\Leuk study re-designed\\C-NMC\\High imbalance\\Train - 1 to 102 ratio\\enhanched'\n",
    "#VAL_PATH = r'../input/5x-aug/Aug_5x/val'\n",
    "BATCH_SIZE=10\n",
    "r = 4\n",
    "c = 4\n",
    "#'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3206d41e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_center(img, bounding):\n",
    "    start = tuple(map(lambda a, da: a//2-da//2, img.shape, bounding))\n",
    "    end = tuple(map(operator.add, start, bounding))\n",
    "    slices = tuple(map(slice, start, end))\n",
    "    return img[slices]\n",
    "\n",
    "def crop_generator(batches, crop_length):\n",
    "    while True:\n",
    "        batch_x, batch_y = next(batches)\n",
    "        batch_crops = np.zeros((batch_x.shape[0], crop_length, crop_length, 3))\n",
    "        for i in range(batch_x.shape[0]):\n",
    "            batch_crops[i] = crop_center(batch_x[i], (crop_length, crop_length))\n",
    "        yield (batch_crops, batch_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30d35f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''train_data = []\n",
    "i = 0\n",
    "plt.figure(figsize=(15, 15))\n",
    "for category in CATEGORIES:\n",
    "    path = os.path.join(TRAIN_DIRECTORY, category)\n",
    "    for img in os.listdir(path):\n",
    "        img_path = os.path.join(path, img)\n",
    "        label = CATEGORIES.index(category)\n",
    "        arr = cv2.imread(img_path)\n",
    "        crop_arr = crop_center(arr, (height,width))\n",
    "        if 1 <= i+1 <= 140:                      # total 140 image\n",
    "            ax = plt.subplot(13, 11, i+1)\n",
    "        plt.imshow(crop_arr)\n",
    "        i += 1\n",
    "        train_data.append([crop_arr, label])\n",
    "        \n",
    "print(len(train_data))'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0086bcb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Special treatment for 5xAug and 10xAug\n",
    "#'''\n",
    "train_datagen = ImageDataGenerator(rescale=1.0/255.0)\n",
    "                                  #featurewise_center=True,\n",
    "                                  #featurewise_std_normalization=True,\n",
    "                                  #validation_split=0.1)\n",
    "train_batches = train_datagen.flow_from_directory(TRAIN_PATH,\n",
    "                                                  class_mode='binary', \n",
    "                                                  color_mode=\"rgb\", \n",
    "                                                  batch_size=BATCH_SIZE, \n",
    "                                                  target_size=(450, 450),\n",
    "                                                  shuffle=True,\n",
    "                                                  seed=42\n",
    "                                                  )\n",
    "\n",
    "train_crops = crop_generator(train_batches, crop)\n",
    "#'''\n",
    "\n",
    "'''\n",
    "train_datagen = ImageDataGenerator(rescale=1.0/255.0)\n",
    "train_batches = train_datagen.flow_from_directory(TRAIN_PATH,\n",
    "                                                  class_mode='binary', \n",
    "                                                  color_mode=\"rgb\", \n",
    "                                                  batch_size=BATCH_SIZE, \n",
    "                                                  target_size=(210, 210),\n",
    "                                                  shuffle=True,\n",
    "                                                  seed=42\n",
    "                                                  )\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f93932b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#'''\n",
    "train_crops\n",
    "x , y = next(train_crops)\n",
    "plt.figure(figsize=(15,15))\n",
    "i=0\n",
    "for img in x:\n",
    "    plt.subplot(r,c,i+1)\n",
    "    plt.imshow(img)\n",
    "    i+=1\n",
    "#'''\n",
    "'''\n",
    "\n",
    "train_batches\n",
    "x , y = next(train_batches)\n",
    "print(x.shape)\n",
    "plt.figure(figsize=(15,15))\n",
    "i=0\n",
    "for img in x:\n",
    "    plt.subplot(r,c,i+1)\n",
    "    plt.imshow(img)\n",
    "    i+=1\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59ab297c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import skimage\n",
    "from skimage.io import imread, imshow\n",
    "import numpy as np\n",
    "\n",
    "img = imread(r'F:\\Leuk study re-designed\\C-NMC\\Low imbalance\\Test\\enhanched\\hem\\4.bmp')\n",
    "imshow(img)\n",
    "print(np.max(img), np.min(img))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fcdb0e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''random.shuffle(train_data)\n",
    "\n",
    "x_train = []\n",
    "y_train = []\n",
    "\n",
    "for features, label in train_data:\n",
    "    x_train.append(features)\n",
    "    y_train.append(label)\n",
    "\n",
    "    \n",
    "x_train = np.array(x_train)\n",
    "y_train = np.array(y_train)\n",
    "print('Before')\n",
    "print(x_train.shape, x_train.dtype, np.max(x_train), np.min(x_train))\n",
    "print(y_train.shape, y_train.dtype, np.max(y_train), np.min(y_train))\n",
    "print('-'*40)\n",
    "\n",
    "\n",
    "#x_train = x_train.astype('float32')\n",
    "#y_train = y_train.astype('float32')\n",
    "\n",
    "x_train = x_train / 255.0\n",
    "\n",
    "print('After')\n",
    "print(x_train.shape, x_train.dtype, np.max(x_train), np.min(x_train))\n",
    "print(y_train.shape, y_train.dtype, np.max(y_train), np.min(y_train))\n",
    "print('-'*40)\n",
    "\n",
    "# np.save(\"x_train_leuknet\",x_train)\n",
    "# np.save(\"y_train_leuknet\",y_train)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96687333",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''all_img = 0\n",
    "for x in y_train:\n",
    "    if x==1:\n",
    "        all_img = all_img + 1\n",
    "print(all_img)'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e2e1ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Model, Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D\n",
    "#from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers import Input, GlobalAveragePooling2D\n",
    "\n",
    "'''\n",
    "import tensorflow as tf\n",
    "base_model = tf.keras.applications.VGG16(  \n",
    "    include_top=False,\n",
    "    weights='imagenet',\n",
    "    input_tensor=None,\n",
    "    input_shape=(height,width,3),\n",
    "    pooling=None,\n",
    "    classes=1,\n",
    "    classifier_activation=\"sigmoid\",\n",
    ")\n",
    "x = base_model.output\n",
    "x = Flatten()(x)\n",
    "x = Dense(4096, activation=tf.nn.sigmoid)(x)\n",
    "x = Dense(4096, activation=tf.nn.sigmoid)(x)\n",
    "prediction = Dense(1, activation=tf.nn.sigmoid)(x)\n",
    "\n",
    "model = Model(inputs=base_model.input,outputs=prediction)\n",
    "\n",
    "model.summary()\n",
    "'''\n",
    "\n",
    "\n",
    "#'''\n",
    "import tensorflow as tf\n",
    "base_model = tf.keras.applications.ResNet50(\n",
    "    include_top=False,\n",
    "    weights='imagenet',\n",
    "    input_tensor=None,\n",
    "    input_shape=(height,width,3),\n",
    "    pooling=None,\n",
    "    classes=1,\n",
    "    classifier_activation=\"sigmoid\",\n",
    ")\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "prediction = Dense(1, activation=tf.nn.sigmoid)(x)\n",
    "\n",
    "model = Model(inputs=base_model.input,outputs=prediction)\n",
    "\n",
    "model.summary()\n",
    "#'''\n",
    "\n",
    "'''\n",
    "from keras.applications import DenseNet121\n",
    "\n",
    "base_model = DenseNet121(weights=None, \n",
    "                         include_top=False, \n",
    "                         input_tensor=None,\n",
    "                         input_shape=(height,width,3),\n",
    "                        )\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "# x = Dense(1024, kernel_regularizer=l2(0.0001), bias_regularizer=l2(0.0001))(x)\n",
    "# x = BatchNormalization()(x)\n",
    "# x = Activation(\"relu\")(x)\n",
    "# x = Dropout(0.5)(x)\n",
    "# x = Dense(512, kernel_regularizer=l2(0.0001), bias_regularizer=l2(0.0001))(x)\n",
    "# x = BatchNormalization()(x)\n",
    "# x = Activation(\"relu\")(x)\n",
    "# x = Dropout(0.5)(x)\n",
    "prediction = Dense(1, activation=tf.nn.sigmoid)(x)\n",
    "\n",
    "model = Model(inputs=base_model.input,outputs=prediction)\n",
    "model.summary()\n",
    "'''\n",
    "\n",
    "\n",
    "'''\n",
    "def get_model():\n",
    "    base_model = tf.keras.applications.EfficientNetB0(\n",
    "        include_top=False,\n",
    "        weights='imagenet',\n",
    "        input_tensor=None,\n",
    "        input_shape=(height,width,3), \n",
    "        pooling=None,\n",
    "        classes=1,\n",
    "        classifier_activation=\"sigmoid\",\n",
    "    )\n",
    "    x = base_model.output\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    prediction = Dense(1, activation=tf.nn.sigmoid)(x)\n",
    "\n",
    "    model = Model(inputs=base_model.input,outputs=prediction)\n",
    "    #model.compile(loss=[mcc_loss], optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "model = get_model()\n",
    "model.summary()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "290b5242",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import tensorflow_addons as tfa\n",
    "# def MCC_LOSS(y_true, y_pred):\n",
    "#     mcc = tfa.metrics.MatthewsCorrelationCoefficient(num_classes=2)\n",
    "#     mccloss = mcc.update_state(y_true, y_pred)\n",
    "#     return (1-mccloss)\n",
    "#'''from keras import backend as K\n",
    "def f1_loss(y_true, y_pred):\n",
    "    \n",
    "    tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=0)\n",
    "    tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=0)\n",
    "    fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=0)\n",
    "    fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=0)\n",
    "\n",
    "    p = tp / (tp + fp + K.epsilon())\n",
    "    r = tp / (tp + fn + K.epsilon())\n",
    "\n",
    "    f1 = 2*p*r / (p+r+K.epsilon())\n",
    "    f1 = tf.where(tf.math.is_nan(f1), tf.zeros_like(f1), f1)\n",
    "    return 1 - K.mean(f1)\n",
    "#'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1069e782",
   "metadata": {},
   "outputs": [],
   "source": [
    "#'''\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "def mcc_loss(y_true, y_pred):\n",
    "    \n",
    "    tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=0)\n",
    "    tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=0)\n",
    "    fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=0) * 1e2\n",
    "    fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=0) / 1e2\n",
    "    \n",
    "    up = tp*tn - fp*fn\n",
    "    down = K.sqrt((tp+fp) * (tp+fn) * (tn+fp) * (tn+fn))\n",
    "    \n",
    "    mcc = up / (down + K.epsilon())\n",
    "    mcc = tf.where(tf.math.is_nan(mcc), tf.zeros_like(mcc), mcc)\n",
    "    \n",
    "    return 1 - K.mean(mcc)\n",
    "#'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d38110f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Modified MCC'''\n",
    "#'''\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "def mod_mcc_loss(y_true, y_pred):\n",
    "    \n",
    "    tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=0)\n",
    "    tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=0)\n",
    "    fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=0) * 1e2\n",
    "    fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=0) / 1e2\n",
    "    \n",
    "    miss_err = fp + fn / tp + tn + fp + fn\n",
    "    \n",
    "    up = tp*tn - fp*fn\n",
    "    down = K.sqrt((tp+fp) * (tp+fn) * (tn+fp) * (tn+fn))\n",
    "    \n",
    "    mcc = (up*miss_err) / (down + K.epsilon())\n",
    "    mcc = tf.where(tf.math.is_nan(mcc), tf.zeros_like(mcc), mcc)\n",
    "    \n",
    "    return 1 - K.mean(mcc)\n",
    "#'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9d862622",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "#from focal_loss import BinaryFocalLoss\n",
    "#from tf.keras.optimizers import Adam, RMSprop, SGD\n",
    "adam_opt = tf.keras.optimizers.Adam(learning_rate=1e-3, beta_1=0.0, beta_2=0.0, amsgrad=True)\n",
    "#adam_opt = tf.keras.optimizers.Adam(lr=1e-6, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=1e-4)\n",
    "#sgd_opt = tf.keras.optimizers.SGD(lr=1e-06, momentum=0.9, decay=0.0, nesterov=False)\n",
    "#rmsp_opt = tf.keras.optimizers.RMSprop(lr=1e-4, decay=0.9)\n",
    "# eve_opt = Eve(lr=1e-4, decay=1E-4, beta_1=0.9, beta_2=0.999, beta_3=0.999, small_k=0.1, big_K=10, epsilon=1e-08)\n",
    "\n",
    "model.compile(optimizer= adam_opt,\n",
    "              #loss = 'binary_crossentropy',\n",
    "              #loss = [binary_focal_loss(alpha=.25, gamma=2)],\n",
    "              #loss = [f1_loss],\n",
    "              #loss = [mcc_loss],\n",
    "              loss = [mod_mcc_loss],\n",
    "              #loss = tfa.losses.SigmoidFocalCrossEntropy(),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8da0e744",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import keras\n",
    "import tensorflow.keras as keras\n",
    "#from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "import tensorflow as tf  \n",
    "best_model_name = best_model_name\n",
    "callbacks = [\n",
    "    tf.keras.callbacks.ModelCheckpoint(best_model_name, monitor='accuracy', save_best_only=True, mode='max'),\n",
    "    tf.keras.callbacks.ReduceLROnPlateau(monitor='accuracy', factor=factor, verbose=1, patience=patience, mode='max')]\n",
    "    #tf.keras.callbacks.EarlyStopping(patience=5, verbose=1)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f33455ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tf.version.VERSION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeacc0ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57b1c1bd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''history = model.fit(x_train, y_train, validation_split=0.1, shuffle=True, batch_size=10, epochs=100, callbacks = callbacks)'''\n",
    "\n",
    "history  = model.fit(train_crops, \n",
    "         steps_per_epoch=train_batches.n//train_batches.batch_size,\n",
    "         #validation_data=val_crops, \n",
    "         #validation_steps=val_batches.n//val_batches.batch_size,   \n",
    "         epochs=epoch,   \n",
    "         verbose=1,     \n",
    "         #class_weight={0:5.51331361, 1:0.54986722},   \n",
    "         callbacks = callbacks)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3564e534",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'])                                                                                                                                                                        \n",
    "#plt.plot(history.history['val_loss'])                                                                                                                                                                                                                                                                                                                              \n",
    "plt.title('Model loss')                                                                                                                                     \n",
    "plt.ylabel('Binary_crossEntropy')                                                                                                                                                                     \n",
    "plt.xlabel('Epoch')                                                                                         \n",
    "plt.legend(['Train', 'Validation'], loc='upper left')          \n",
    "plt.show()           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c059d837",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['accuracy'])                                            \n",
    "#plt.plot(history.history['val_accuracy'])         \n",
    "plt.title('Model accuracy')                          \n",
    "plt.ylabel('Accuracy')                          \n",
    "plt.xlabel('Epoch')                        \n",
    "plt.legend(['Train', 'Validation'], loc='upper left')    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08320372",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Lowest training loss: ', min(history.history['loss']))    \n",
    "# print('Lowest validation loss: ', min(history.history['val_loss']))         \n",
    "print('Highest training accuracy: ', max(history.history['accuracy']))           \n",
    "# print('Highest validation accuracy: ', max(history.history['val_accuracy']))                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f371fb95",
   "metadata": {},
   "outputs": [],
   "source": [
    "#'''\n",
    "test_all_path = r'F:\\Leuk study re-designed\\C-NMC\\High imbalance\\Test\\enhanched\\all'\n",
    "test_hem_path = r'F:\\Leuk study re-designed\\C-NMC\\High imbalance\\Test\\enhanched\\hem'\n",
    "\n",
    "test_all_list = os.listdir(test_all_path)\n",
    "test_hem_list = os.listdir(test_hem_path)\n",
    "\n",
    "pred_list = []\n",
    "for x in range(len(test_hem_list)):\n",
    "    img = imread(os.path.join(test_hem_path, test_hem_list[x]))\n",
    "    #imshow(img)\n",
    "    img =  crop_center(img, (height,width))\n",
    "    #imshow(img)\n",
    "    img = img.astype('float32')\n",
    "    img = img / 255.0\n",
    "    img = np.expand_dims(img, axis=0)\n",
    "    pred = model(img)\n",
    "#     print(pred.numpy()[0][0])\n",
    "#     break\n",
    "    pred_list.append(pred.numpy()[0][0])\n",
    "print(pred_list)\n",
    "#'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b55e0dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#'''\n",
    "np.unique(np.array(pred_list))\n",
    "#'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08d8a41c",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''np.unique(np.array(pred_list))'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2f3b4523",
   "metadata": {},
   "outputs": [],
   "source": [
    "#a"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4de7d14b",
   "metadata": {},
   "source": [
    "# Testing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99272070",
   "metadata": {},
   "outputs": [],
   "source": [
    "# height = 150\n",
    "# width = 150\n",
    "\n",
    "val_all_path = r'F:\\Leuk study re-designed\\C-NMC\\High imbalance\\Test\\enhanched\\all'\n",
    "val_hem_path = r'F:\\Leuk study re-designed\\C-NMC\\High imbalance\\Test\\enhanched\\hem'\n",
    "\n",
    "val_all_list = os.listdir(val_all_path)\n",
    "#val_all_list.sort()\n",
    "\n",
    "val_hem_list = os.listdir(val_hem_path)          \n",
    "#val_hem_list.sort()\n",
    "\n",
    "\n",
    "print('val/all_list_length: ', len(val_all_list))\n",
    "print('val/hem_list_length :', len(val_hem_list))\n",
    "\n",
    "val_all_batch = np.zeros((len(val_all_list), height, width, 3), dtype=np.uint8)\n",
    "val_hem_batch = np.zeros((len(val_hem_list), height, width, 3), dtype=np.uint8)\n",
    "\n",
    "print('val_all_batch shape: ', val_all_batch.shape, 'val_hem_batch shape: ', val_hem_batch.shape)\n",
    "\n",
    "\n",
    "def Read_n_Crop(list_data, batch, path):\n",
    "    i=0\n",
    "    for x in list_data:\n",
    "        image = cv2.imread(os.path.join(path, x))\n",
    "        image = cv2.cvtColor(image,cv2.COLOR_BGR2RGB)\n",
    "        image = crop_center(image, (height,width,3))\n",
    "        batch[i] = image\n",
    "        i+=1\n",
    "    \n",
    "    print('batch type: ', type(batch), 'batch shape: ', batch.shape, 'batch dtype: ', batch.dtype, 'batch[0] shape: ', batch[0].shape, 'batch[0] dtype: ', batch[0].dtype)\n",
    "    return batch\n",
    "\n",
    "\n",
    "def crop_center(img, bounding):\n",
    "    start = tuple(map(lambda a, da: a//2-da//2, img.shape, bounding))\n",
    "    end = tuple(map(operator.add, start, bounding))\n",
    "    slices = tuple(map(slice, start, end))\n",
    "    return img[slices]\n",
    "\n",
    "parasite_images=Read_n_Crop(val_all_list, val_all_batch, val_all_path)\n",
    "uninf_images=Read_n_Crop(val_hem_list, val_hem_batch, val_hem_path)\n",
    "\n",
    "\n",
    "para_label = np.array([0 for _ in range(len(parasite_images))])\n",
    "uninf_label = np.array([1 for _ in range(len(uninf_images))])\n",
    "\n",
    "print('parasite/all_label shape: ', para_label.shape, 'uninf_label shape: ', uninf_label.shape)\n",
    "\n",
    "x_all = np.concatenate((parasite_images, uninf_images), axis=0)\n",
    "y_all = np.concatenate((para_label, uninf_label), axis=0)\n",
    "print('After concatenation.............................')\n",
    "print('x_all shape: ', x_all.shape, 'y_all shape: ', y_all.shape)\n",
    "\n",
    "########\n",
    "'''Model imports here'''\n",
    "#model = tf.keras.models.load_model(best_model_name)\n",
    "model.load_weights(os.path.join(os.getcwd(), best_model_name))\n",
    "########\n",
    "\n",
    "x_all=x_all/255.0\n",
    "# Make predictions using trained model\n",
    "y_pred = model.predict(x_all, verbose=1)\n",
    "print(\"Predictions: \", y_pred.shape)\n",
    "\n",
    "y_pred_flat = []\n",
    "for pred in y_pred:\n",
    "    if pred > 0.5:\n",
    "        y_pred_flat.append(1)\n",
    "    else:\n",
    "        y_pred_flat.append(0)\n",
    "y_pred_flat = np.array(y_pred_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "762627d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.path.join(os.getcwd(), best_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95e50d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Samples classified as all / 1219: ', y_pred_flat.tolist().count(0))\n",
    "print('Samples classified as hem / 648: ', y_pred_flat.tolist().count(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b522f7b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(y_pred_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38d2bac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_pred_flat.tolist().count(0), y_pred_flat.tolist().count(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "189bf5b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "# Classification report\n",
    "\n",
    "confusion_mtx = confusion_matrix(y_all, y_pred_flat) \n",
    "print(confusion_mtx)\n",
    "target_names = ['0', '1']\n",
    "report = classification_report(y_all, y_pred_flat, target_names=target_names, digits=4)\n",
    "print(classification_report(y_all, y_pred_flat, target_names=target_names, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9a6d6d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "report_list = report.split()\n",
    "for x in range(len(report_list)):\n",
    "    if report_list[x] == 'accuracy':\n",
    "        acc = report_list[x+1]\n",
    "    elif report_list[x] == 'weighted':\n",
    "        precision = report_list[x+2]\n",
    "        recall = report_list[x+3]\n",
    "        f1 = report_list[x+4]\n",
    "print(acc, precision, recall, f1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df0b5cd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "report_list = report.split()\n",
    "for x in range(len(report_list)):\n",
    "    if report_list[x] == 'accuracy':\n",
    "        acc = report_list[x+1]\n",
    "    elif report_list[x] == 'weighted':\n",
    "        precision = report_list[x+2]\n",
    "        recall = report_list[x+3]\n",
    "        f1 = report_list[x+4]\n",
    "print(acc, precision, recall, f1)\n",
    "\n",
    "log_path = 'F:\\Leuk study re-designed\\Log\\C-NMC\\High imbalnce'\n",
    "filename = best_model_name[:-2] + 'txt'#'baseline_run_3.txt'\n",
    "f = open(os.path.join(log_path, filename), \"w\")\n",
    "\n",
    "content = best_model_name + '\\n\\n'  + 'Location: old pc 64gb' + '\\n' + 'Low Imbalance' + '\\n' + 'Train-path=enhanched' + '\\n' + 'Test-path=enhanched' + '\\n\\n' + 'height: ' + str(height) + '\\t' + 'width: ' + str(width) + '\\t' + 'crop: ' + str(crop) + '\\n\\n' + 'factor: ' + str(factor) + '\\t' + 'patience: ' + str(patience) + '\\t' + 'epoch: ' + str(epoch) + '\\n\\n' +'Lowest Training Loss: ' + str(min(history.history['loss'])) + '\\n' +'Highest training accuracy: ' + str(max(history.history['accuracy'])) + '\\n\\n' + 'Accuracy: ' + str(acc) + '\\t' + 'Precision: ' + str(precision) + '\\t' + 'Recall: ' + str(recall) + '\\t' + 'F1-score: ' + str(f1) + '\\n\\n' + 'all: ' + str(confusion_mtx[0]) + '\\n\\n' + 'hem: ' + str(confusion_mtx[1]) +  '\\n      all hem' \n",
    "            \n",
    "            \n",
    " \n",
    "f.write(content)\n",
    "f.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b443a7a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# conda install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32518d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "# prediction = le.inerse_transform(y_pred)\n",
    "# confusion_mtx = confusion_matrix(y_all, prediction) \n",
    "print(confusion_mtx)\n",
    "sns.heatmap(confusion_mtx, annot=True,fmt=\".3g\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfc537ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# conda uninstall seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a838ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crop some images\n",
    "\n",
    "import skimage\n",
    "import os\n",
    "from skimage.io import imread, imshow, imsave\n",
    "path = r\"F:\\New folder\"\n",
    "item_list = os.listdir(path)\n",
    "item_list.sort()\n",
    "print(len(item_list))\n",
    "des_path = r\"F:/New folder/\"\n",
    "for x in range(len(item_list)):\n",
    "    img = imread(os.path.join(path, item_list[x]))\n",
    "    cropped = crop_center(img, (210,210,3))\n",
    "    new_path_crop = des_path + item_list[x]\n",
    "    new_path_orig = des_path + 'cropped' + item_list[x] \n",
    "    imsave(new_path_crop,cropped)\n",
    "    imsave(new_path_orig, img)\n",
    "    if x==10:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2044a0df",
   "metadata": {},
   "outputs": [],
   "source": [
    "imshow(cropped)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "leukemia",
   "language": "python",
   "name": "leukemia"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf \n",
    "physical_devices = tf.config.list_physical_devices('GPU') \n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install tensorflow-addons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import operator\n",
    "import tensorflow as tf\n",
    "import random\n",
    "from keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "import skimage\n",
    "from skimage.io import imread,imshow\n",
    "path = r'D:\\new_leuk\\Leukemia_Work_Revive\\Autoencoder oversample data\\val\\all'\n",
    "\n",
    "im_list = os.listdir(path)\n",
    "len(im_list)\n",
    "\n",
    "for x in range(len(im_list)):\n",
    "    im = imread(os.path.join(path, im_list[x]))\n",
    "    maxi = np.max(im)\n",
    "    mini = np.min(im)\n",
    "    if str(im.dtype) != 'uint8':\n",
    "        print(path+im_list[x])\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "TRAIN_PATH = r'D:\\new_leuk\\Leukemia_Work_Revive\\Corrected\\Autoencoder oversampe data\\train'\n",
    "VAL_PATH = r'D:\\new_leuk\\Leukemia_Work_Revive\\Corrected\\Autoencoder oversampe data\\val'\n",
    "BATCH_SIZE=10\n",
    "r = 4\n",
    "c = 4\n",
    "'''\n",
    "\n",
    "'''\n",
    "TRAIN_PATH = r'D:\\new_leuk\\Leukemia_Work_Revive\\Corrected\\Aug_5x\\train'\n",
    "VAL_PATH = r'D:\\new_leuk\\Leukemia_Work_Revive\\Corrected\\Aug_5x\\val'\n",
    "BATCH_SIZE=10\n",
    "r = 4\n",
    "c = 4\n",
    "'''\n",
    "# 10xAug 210 x 210 x 3\n",
    "#'''\n",
    "TRAIN_PATH = r'D:\\new_leuk\\Leukemia_Work_Revive\\Corrected\\Aug_10x\\train'\n",
    "VAL_PATH = r'D:\\new_leuk\\Leukemia_Work_Revive\\Corrected\\Aug_10x\\val'\n",
    "BATCH_SIZE=4\n",
    "r = 2\n",
    "c = 2\n",
    "#'''\n",
    "\n",
    "'''\n",
    "TRAIN_PATH = r'D:\\new_leuk\\Leukemia_Work_Revive\\Corrected\\Oversampled minority class\\train'\n",
    "VAL_PATH = r'D:\\new_leuk\\Leukemia_Work_Revive\\Corrected\\Oversampled minority class\\val'\n",
    "BATCH_SIZE=10\n",
    "r = 4\n",
    "c = 4\n",
    "'''\n",
    "\n",
    "'''\n",
    "TRAIN_PATH = r'D:\\new_leuk\\Leukemia_Work_Revive\\Corrected\\Originall data\\train'\n",
    "VAL_PATH = r'D:\\new_leuk\\Leukemia_Work_Revive\\Corrected\\Originall data\\val'\n",
    "BATCH_SIZE=4\n",
    "r = 2\n",
    "c = 2\n",
    "'''\n",
    "\n",
    "\n",
    "'''\n",
    "TRAIN_PATH = r'D:\\new_leuk\\Leukemia_Work_Revive\\Corrected\\CycleGanOverSampled Data\\train'\n",
    "VAL_PATH = r'D:\\new_leuk\\Leukemia_Work_Revive\\Corrected\\CycleGanOverSampled Data\\val'\n",
    "BATCH_SIZE=4\n",
    "r = 2\n",
    "c = 2\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_center(img, bounding):\n",
    "    start = tuple(map(lambda a, da: a//2-da//2, img.shape, bounding))\n",
    "    end = tuple(map(operator.add, start, bounding))\n",
    "    slices = tuple(map(slice, start, end))\n",
    "    im = img[slices].astype('float32')\n",
    "    return im\n",
    "\n",
    "\n",
    "def crop_generator(batches, crop_length):\n",
    "    while True:\n",
    "        batch_x, batch_y = next(batches)\n",
    "        batch_crops = np.zeros((batch_x.shape[0], crop_length, crop_length, 3))\n",
    "        for i in range(batch_x.shape[0]):\n",
    "            batch_crops[i] = crop_center(batch_x[i], (crop_length, crop_length))\n",
    "        yield (batch_crops, batch_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#'''\n",
    "train_datagen = ImageDataGenerator(rescale=1.0/255.0)\n",
    "train_batches = train_datagen.flow_from_directory(TRAIN_PATH,\n",
    "                                                  class_mode='binary', \n",
    "                                                  color_mode=\"rgb\", \n",
    "                                                  batch_size=BATCH_SIZE, \n",
    "                                                  target_size=(210, 210),\n",
    "                                                  shuffle=True,\n",
    "                                                  seed=42\n",
    "                                                  )\n",
    "\n",
    "train_crops = crop_generator(train_batches, 150)\n",
    "#'''\n",
    "'''\n",
    "\n",
    "train_datagen = ImageDataGenerator(rescale=1.0/255.0)\n",
    "train_batches = train_datagen.flow_from_directory(TRAIN_PATH,\n",
    "                                                  class_mode='binary', \n",
    "                                                  color_mode=\"rgb\", \n",
    "                                                  batch_size=BATCH_SIZE, \n",
    "                                                  target_size=(210, 210),\n",
    "                                                  shuffle=True,\n",
    "                                                  seed=42\n",
    "                                                  )\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#'''\n",
    "train_crops\n",
    "x , y = next(train_crops)\n",
    "plt.figure(figsize=(15,15))\n",
    "i=0\n",
    "for img in x:\n",
    "    plt.subplot(r,c,i+1)\n",
    "    plt.imshow(img)\n",
    "    i+=1\n",
    "#'''\n",
    "\n",
    "\n",
    "'''\n",
    "train_batches\n",
    "x , y = next(train_batches)\n",
    "print(x.shape)\n",
    "plt.figure(figsize=(15,15))\n",
    "i=0\n",
    "for img in x:\n",
    "    plt.subplot(r,c,i+1)\n",
    "    plt.imshow(img)\n",
    "    i+=1\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#'''\n",
    "val_datagen = ImageDataGenerator(rescale=1.0/255.0)\n",
    "val_batches = val_datagen.flow_from_directory(VAL_PATH,\n",
    "                                                class_mode='binary', \n",
    "                                                color_mode=\"rgb\", \n",
    "                                                batch_size=BATCH_SIZE, \n",
    "                                                target_size=(210, 210),\n",
    "                                                shuffle=True,\n",
    "                                                seed=42\n",
    "                                                )\n",
    "\n",
    "val_crops = crop_generator(val_batches, 150)\n",
    "#'''\n",
    "'''\n",
    "\n",
    "val_datagen = ImageDataGenerator(rescale=1.0/255.0)\n",
    "val_batches = val_datagen.flow_from_directory(VAL_PATH,\n",
    "                                                class_mode='binary', \n",
    "                                                color_mode=\"rgb\", \n",
    "                                                batch_size=BATCH_SIZE, \n",
    "                                                target_size=(210, 210),\n",
    "                                                shuffle=True,\n",
    "                                                seed=42\n",
    "                                                )\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#'''\n",
    "val_crops\n",
    "x , y = next(val_crops)\n",
    "plt.figure(figsize=(15,15))\n",
    "i=0\n",
    "for img in x:\n",
    "    plt.subplot(r,c,i+1)\n",
    "    plt.imshow(img)\n",
    "    i+=1\n",
    "#'''\n",
    "\n",
    "'''\n",
    "val_batches\n",
    "x , y = next(val_batches)\n",
    "print(x.shape)\n",
    "plt.figure(figsize=(15,15))\n",
    "i=0\n",
    "for img in x:\n",
    "    plt.subplot(r,c,i+1)\n",
    "    plt.imshow(img)\n",
    "    i+=1\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "import tensorflow as tf\n",
    "\n",
    "resnet = tf.keras.Sequential([\n",
    "    tf.keras.layers.Input(shape=(224,224,3)),\n",
    "    tf.keras.applications.ResNet50(\n",
    "        weights='imagenet', \n",
    "        input_shape=(224,224,3),\n",
    "        pooling=\"avg\",\n",
    "        classifier_activation=None\n",
    "        )\n",
    "    ])\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "#import tensorflow.keras as keras\n",
    "#import tensorflow.keras as keras\n",
    "from tensorflow.keras.applications import VGG16\n",
    "model = VGG16(\n",
    "    include_top=True,\n",
    "    weights=None,\n",
    "    input_tensor=None,\n",
    "    input_shape=(210,210,3),\n",
    "    pooling=None,\n",
    "    classes=2,\n",
    "    classifier_activation='softmax'\n",
    ")\n",
    "model.summary()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Model, Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers import Input, GlobalAveragePooling2D\n",
    "\n",
    "'''\n",
    "import tensorflow as tf\n",
    "base_model = tf.keras.applications.VGG16(  \n",
    "    include_top=False,\n",
    "    weights='imagenet',\n",
    "    input_tensor=None,\n",
    "    input_shape=(150,150,3),\n",
    "    pooling=None,\n",
    "    classes=1,\n",
    "    classifier_activation=\"sigmoid\",\n",
    ")\n",
    "x = base_model.output\n",
    "x = Flatten()(x)\n",
    "x = Dense(4096, activation=tf.nn.sigmoid)(x)\n",
    "x = Dense(4096, activation=tf.nn.sigmoid)(x)\n",
    "prediction = Dense(1, activation=tf.nn.sigmoid)(x)\n",
    "\n",
    "model = Model(inputs=base_model.input,outputs=prediction)\n",
    "\n",
    "model.summary()\n",
    "'''\n",
    "\n",
    "#'''\n",
    "import tensorflow as tf\n",
    "base_model = tf.keras.applications.ResNet50(\n",
    "    include_top=False,\n",
    "    weights='imagenet',\n",
    "    input_tensor=None,\n",
    "    input_shape=(150,150,3),\n",
    "    pooling=None,\n",
    "    classes=1,\n",
    "    classifier_activation=\"sigmoid\",\n",
    ")\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "prediction = Dense(1, activation=tf.nn.sigmoid)(x)\n",
    "\n",
    "model = Model(inputs=base_model.input,outputs=prediction)\n",
    "\n",
    "model.summary()\n",
    "#'''\n",
    "\n",
    "\n",
    "'''\n",
    "import tensorflow as tf\n",
    "model = tf.keras.applications.ResNet50(\n",
    "    include_top=True,\n",
    "    weights='imagenet',\n",
    "    input_tensor=None,\n",
    "    input_shape=(150,150,3),\n",
    "    pooling=None,\n",
    "    classes=1,\n",
    "    classifier_activation=\"sigmoid\",\n",
    ")\n",
    "model.summary()\n",
    "'''\n",
    "\n",
    "'''\n",
    "import os\n",
    "import numpy as np\n",
    "np.random.seed(777)\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.optimizers import Adam, SGD, RMSprop\n",
    "\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers import Input, GlobalAveragePooling2D\n",
    "\n",
    "from keras.regularizers import l2\n",
    "from keras.models import load_model\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "from keras.applications import DenseNet121\n",
    "\n",
    "base_model = DenseNet121(weights=None, \n",
    "                         include_top=False, \n",
    "                         input_tensor=None,\n",
    "                         input_shape=(210,210,3),\n",
    "                        )\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "# x = Dense(1024, kernel_regularizer=l2(0.0001), bias_regularizer=l2(0.0001))(x)\n",
    "# x = BatchNormalization()(x)\n",
    "# x = Activation(\"relu\")(x)\n",
    "# x = Dropout(0.5)(x)\n",
    "# x = Dense(512, kernel_regularizer=l2(0.0001), bias_regularizer=l2(0.0001))(x)\n",
    "# x = BatchNormalization()(x)\n",
    "# x = Activation(\"relu\")(x)\n",
    "# x = Dropout(0.5)(x)\n",
    "prediction = Dense(1, activation=tf.nn.sigmoid)(x)\n",
    "\n",
    "model = Model(inputs=base_model.input,outputs=prediction)\n",
    "model.summary()\n",
    "'''\n",
    "\n",
    "'''\n",
    "model = tf.keras.applications.EfficientNetB0(\n",
    "    include_top=True,\n",
    "    weights=None,\n",
    "    input_tensor=None,\n",
    "    input_shape=(150,150,3),\n",
    "    pooling=None,\n",
    "    classes=1,\n",
    "    classifier_activation=\"sigmoid\",\n",
    ")\n",
    "model.summary()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "import keras\n",
    "from keras.layers import Input, Activation, merge, Dense, Flatten, Dropout, BatchNormalization, GlobalAveragePooling2D\n",
    "\n",
    "def effnet():\n",
    "    # build model\n",
    "    base_model = tf.keras.applications.EfficientNetB0(input_shape=(150,150,3), weights='imagenet', include_top=False)\n",
    "    \n",
    "    # add GAP layer and 2-ways softmax\n",
    "    x = keras.layers.GlobalAveragePooling2D()(base_model.output)\n",
    "    output = keras.layers.Dense(2, activation='sigmoid')(x)\n",
    "    \n",
    "    model = keras.models.Model(inputs=[base_model.input], outputs=[output])\n",
    "    #model.compile(optimizer='SGD', loss=f1_loss, metrics=['accuracy'] )\n",
    "    return model\n",
    "\n",
    "\n",
    "model=None\n",
    "model=effnet()\n",
    "model.summary()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''from keras.models import Model, Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers import Input, GlobalAveragePooling2D\n",
    "def get_model():\n",
    "    base_model = tf.keras.applications.EfficientNetB0(\n",
    "        include_top=False,\n",
    "        weights='imagenet',\n",
    "        input_tensor=None,\n",
    "        input_shape=(150,150,3),\n",
    "        pooling=None,\n",
    "        classes=1,\n",
    "        classifier_activation=\"sigmoid\",\n",
    "    )\n",
    "    x = base_model.output\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    prediction = Dense(1, activation=tf.nn.sigmoid)(x)\n",
    "\n",
    "    model = Model(inputs=base_model.input,outputs=prediction)\n",
    "    #model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    return model\n",
    "model = get_model()\n",
    "model.summary()'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "# define cnn model\n",
    "def define_model():\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same', input_shape=(200, 200, 3)))\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation='relu', kernel_initializer='he_uniform'))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    # compile model\n",
    "    opt = SGD(lr=0.001, momentum=0.9)\n",
    "    model.compile(optimizer=opt, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "model  = define_model()\n",
    "model.summary()\n",
    "from keras.applications import VGG16\n",
    "# include_top=True,\n",
    "# weights=\"imagenet\",\n",
    "#input_tensor=None,\n",
    "#input_shape=None,\n",
    "#pooling=None,\n",
    "#classes=1000,\n",
    "#classifier_activation=\"softmax\",\n",
    "model = VGG16(include_top=True,weights=None,input_tensor=None,input_shape=None,pooling=None,classes=1,classifier_activation=\"sigmoid\",)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tf.compat.v1.enable_eager_execution()\n",
    "import tensorflow as tf\n",
    "tf.executing_eagerly()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# example of tending the vgg16 model\n",
    "#from tensorflow.keras.applications.vgg16 import VGG16\n",
    "#from tensorflow.keras.models import Model\n",
    "#from tensorflow.keras.layers import Dense\n",
    "#from tensorflow.keras.layers import Flatten\n",
    "'''\n",
    "import tensorflow as tf\n",
    "# load model without classifier layers\n",
    "model = tf.keras.applications.VGG16(include_top=False, input_shape=(210, 210, 3))\n",
    "# add new classifier layers\n",
    "flat1 = tf.keras.layers.Flatten()(model.layers[-1].output)\n",
    "class1 = tf.keras.layers.Dense(4096, activation='relu')(flat1)\n",
    "class2 = tf.keras.layers.Dense(4096, activation='relu')(class1)\n",
    "output = tf.keras.layers.Dense(1, activation='sigmoid')(class2)\n",
    "# define new model\n",
    "model = tf.keras.Model(inputs=model.inputs, outputs=output)\n",
    "# summarize\n",
    "model.summary()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "from keras import backend as K\n",
    "def binary_focal_loss(gamma=2, alpha=0.25):\n",
    "    \"\"\"\n",
    "    Binary form of focal loss.\n",
    "         Focal loss for binary classification problems\n",
    "    \n",
    "    focal_loss(p_t) = -alpha_t * (1 - p_t)**gamma * log(p_t)\n",
    "        where p = sigmoid(x), p_t = p or 1 - p depending on if the label is 1 or 0, respectively.\n",
    "    References:\n",
    "        https://arxiv.org/pdf/1708.02002.pdf\n",
    "    Usage:\n",
    "     model.compile(loss=[binary_focal_loss(alpha=.25, gamma=2)], metrics=[\"accuracy\"], optimizer=adam)\n",
    "    \"\"\"\n",
    "    alpha = tf.constant(alpha, dtype=tf.float32)\n",
    "    gamma = tf.constant(gamma, dtype=tf.float32)\n",
    "\n",
    "    def binary_focal_loss_fixed(y_true, y_pred):\n",
    "        \"\"\"\n",
    "        y_true shape need be (None,1)\n",
    "        y_pred need be compute after sigmoid\n",
    "        \"\"\"\n",
    "        y_true = tf.cast(y_true, tf.float32)\n",
    "        alpha_t = y_true*alpha + (K.ones_like(y_true)-y_true)*(1-alpha)\n",
    "    \n",
    "        p_t = y_true*y_pred + (K.ones_like(y_true)-y_true)*(K.ones_like(y_true)-y_pred) + K.epsilon()\n",
    "        focal_loss = - alpha_t * K.pow((K.ones_like(y_true)-p_t),gamma) * K.log(p_t)\n",
    "        return K.mean(focal_loss)\n",
    "    return binary_focal_loss_fixed\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "def f1_loss(y_true, y_pred, smooth=1):\n",
    "    \n",
    "    intersection = tf.reduce_sum(y_true * y_pred, axis=-1)\n",
    "    denominator = tf.reduce_sum(y_true + y_pred, axis=-1)\n",
    "    f1 = (2 * intersection + smooth) / ( denominator + smooth)\n",
    "    \n",
    "    return (1 - f1) * smooth\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "def mcc_loss(y_true, y_pred):\n",
    "    \n",
    "    tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=0)\n",
    "    tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=0)\n",
    "    fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=0) * 1e2\n",
    "    fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=0) / 1e2\n",
    "    \n",
    "    up = tp*tn - fp*fn\n",
    "    down = K.sqrt((tp+fp) * (tp+fn) * (tn+fp) * (tn+fn))\n",
    "    \n",
    "    mcc = up / (down + K.epsilon())\n",
    "    mcc = tf.where(tf.math.is_nan(mcc), tf.zeros_like(mcc), mcc)\n",
    "    \n",
    "    return 1 - K.mean(mcc)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import tensorflow_addons as tfa\n",
    "# def MCC_LOSS(y_true, y_pred):\n",
    "#     mcc = tfa.metrics.MatthewsCorrelationCoefficient(num_classes=2)\n",
    "#     mccloss = mcc.update_state(y_true, y_pred)\n",
    "#     return (1-mccloss)\n",
    "'''from keras import backend as K\n",
    "def f1_loss(y_true, y_pred):\n",
    "    \n",
    "    tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=0)\n",
    "    tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=0)\n",
    "    fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=0)\n",
    "    fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=0)\n",
    "\n",
    "    p = tp / (tp + fp + K.epsilon())\n",
    "    r = tp / (tp + fn + K.epsilon())\n",
    "\n",
    "    f1 = 2*p*r / (p+r+K.epsilon())\n",
    "    f1 = tf.where(tf.math.is_nan(f1), tf.zeros_like(f1), f1)\n",
    "    return 1 - K.mean(f1)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "#from focal_loss import BinaryFocalLoss\n",
    "#from tf.keras.optimizers import Adam, RMSprop, SGD\n",
    "adam_opt = tf.keras.optimizers.Adam(lr=1e-4, beta_1=0.0, beta_2=0.0, amsgrad=True)\n",
    "#adam_opt = tf.keras.optimizers.Adam(lr=1e-3, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=1e-4)\n",
    "#sgd_opt = tf.keras.optimizers.SGD(lr=1e-06, momentum=0.9, decay=0.0, nesterov=False)\n",
    "#sgd_opt = tf.keras.optimizers.SGD(lr=1e-3)\n",
    "#rmsp_opt = tf.keras.optimizers.RMSprop(lr=1e-4, decay=0.9)\n",
    "# eve_opt = Eve(lr=1e-4, decay=1E-4, beta_1=0.9, beta_2=0.999, beta_3=0.999, small_k=0.1, big_K=10, epsilon=1e-08)\n",
    "\n",
    "model.compile(optimizer= adam_opt,\n",
    "              loss = 'binary_crossentropy',\n",
    "              #loss = f1_loss,\n",
    "              #oss = [binary_focal_loss(alpha=.25, gamma=2)],\n",
    "              #oss = [f1_loss],\n",
    "              #loss = [mcc_loss],\n",
    "              #loss = tfa.losses.SigmoidFocalCrossEntropy(),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import keras\n",
    "import tensorflow.keras as keras\n",
    "#from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "import tensorflow as tf\n",
    "callbacks = [\n",
    "    tf.keras.callbacks.ModelCheckpoint('ResNet50_10xAug_test1.h5', monitor='val_accuracy', save_best_only=True, mode='max'),\n",
    "    tf.keras.callbacks.ReduceLROnPlateau(monitor='val_accuracy', factor=0.94, verbose=1, patience=5, mode='max')]\n",
    "    #tf.keras.callbacks.EarlyStopping(patience=5, verbose=1)]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#x, y = next(train_crops)\n",
    "#x = x.astype('float32')\n",
    "#train_crops = yield(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (np.dtype(np.float64).itemsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history  = model.fit(train_crops,                           \n",
    "         steps_per_epoch=train_batches.n//train_batches.batch_size,\n",
    "         validation_data=val_crops, \n",
    "         validation_steps=val_batches.n//val_batches.batch_size, \n",
    "         epochs=500, \n",
    "         #class_weight={0:0.73301705, 1:1.57288286},            \n",
    "         verbose=1,\n",
    "         callbacks = callbacks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'])                                                                                                                                        \n",
    "plt.plot(history.history['val_loss'])                                                                                                                                                                                                                                                     \n",
    "plt.title('Model loss')                                                                                   \n",
    "plt.ylabel('Binary_crossEntropy')                                                                                                                 \n",
    "plt.xlabel('Epoch')                                                                      \n",
    "plt.legend(['Train', 'Validation'], loc='upper left')          \n",
    "plt.show()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['accuracy'])                                            \n",
    "plt.plot(history.history['val_accuracy'])         \n",
    "plt.title('Model accuracy')                           \n",
    "plt.ylabel('Accuracy')                         \n",
    "plt.xlabel('Epoch')                      \n",
    "plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Lowest training loss: ', min(history.history['loss']))\n",
    "print('Lowest validation loss: ', min(history.history['val_loss']))        \n",
    "print('Highest training accuracy: ', max(history.history['accuracy']))\n",
    "print('Highest validation accuracy: ', max(history.history['val_accuracy']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "leukemia",
   "language": "python",
   "name": "leukemia"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
